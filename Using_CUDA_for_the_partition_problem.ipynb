{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pasamv/CIS641-HW2-PASAM/blob/main/Using_CUDA_for_the_partition_problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Yl5Wgx34oa6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A small example of coding with CUDA\n",
        "CUDA is the programming language that NVIDIA created to program its GPUs.\n",
        "\n",
        "There are several key steps in programming a GPU:\n",
        "\n",
        "1. Allocating memory in the GPU card\n",
        "2. Copying data from the host memory to the GPU memory\n",
        "3. Executing code in the GPU\n",
        "4. Copying the results back to the host memory\n",
        "\n",
        "Before continuing, make sure that you have set the runtime environment to a GPU.\n",
        "\n",
        "Choose the Runtime set of commands on top and selecte\n",
        " Change runtime type\n",
        "\n",
        " Choose T4 GPU"
      ],
      "metadata": {
        "id": "tscICGBYocWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CUDA code needs a special compiler from NVIDIA called nvcc.\n",
        "nvcc is installed by default on COLAB."
      ],
      "metadata": {
        "id": "DBKCBAvCqFC8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1HE6NZIoZ7H",
        "outputId": "54d7d344-5c5a-416d-9358-8902801b9310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are interested in the specifications of the GPU, and you have an NVIDIA GPU on your system, the command nvidia-smi displays information about the GPU."
      ],
      "metadata": {
        "id": "5hn7z7I3qOyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-FFKGgwpexd",
        "outputId": "14dc4181-051f-45e3-a273-146a9bba4691"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Feb 18 00:38:53 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the purpose of this example, we will see at an example that solves the partition problem."
      ],
      "metadata": {
        "id": "RTf6MdOkqxyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cudaPartition.cu\n",
        "/*\n",
        " * cudaPartition.cu\n",
        " * Solve the Partition problem using CUDA.\n",
        " * https://en.wikipedia.org/wiki/Partition_problem\n",
        " * This code works for multisets of up to 32 elements\n",
        " * The input is expected to be as follows:\n",
        " * The first line will contain n, the number of elements in the multiset\n",
        " * The remaining n lines will contain the n values, one per line\n",
        " */\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "// The kernel\n",
        "// This function is executed, in parallel, on the processors on the GPU card\n",
        "//\n",
        "__global__\n",
        "void evaluatePartition(  int n, int *array,int *result) {\n",
        "  unsigned int value = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "  int sum0s = 0;\n",
        "  int sum1s = 0;\n",
        "  unsigned int mask = 1;\n",
        "  for(int i = 0;i < n;i++) {\n",
        "    if ((mask & value) != 0) {\n",
        "      sum1s = sum1s + array[i];\n",
        "    }\n",
        "    else {\n",
        "      sum0s = sum0s + array[i];\n",
        "    }\n",
        "    mask = mask * 2;\n",
        "  }\n",
        "  if (sum0s == sum1s)\n",
        "     result[value] = 1;\n",
        "  else\n",
        "     result[value] = 0;\n",
        "  // printf(\"%d %d \\n\",value,result[value]);\n",
        "}\n",
        "\n",
        "void printResults(unsigned int value,int n,int *array)\n",
        "{\n",
        "  printf(\"Solution:\\n\");\n",
        "  printf(\"First partition: \") ;\n",
        "  unsigned int mask = 1;\n",
        "  int sum = 0;\n",
        "  for(int i = 0;i < n;i++) {\n",
        "    if ((mask & value) != 0) {\n",
        "      printf(\"%d \",array[i]);\n",
        "      sum = sum + array[i];\n",
        "    }\n",
        "    mask = mask * 2;\n",
        "  }\n",
        "  printf(\" sum: %d \\n\",sum);\n",
        "  printf(\"Second partition: \") ;\n",
        "  mask = 1;\n",
        "  sum = 0;\n",
        "  for(int i = 0;i < n;i++) {\n",
        "    if ((mask & value) == 0) {\n",
        "      printf(\"%d \",array[i]);\n",
        "      sum = sum + array[i];\n",
        "    }\n",
        "    mask = mask * 2;\n",
        "  }\n",
        "  printf(\" sum: %d \\n\",sum);\n",
        "}\n",
        "\n",
        "\n",
        "int main() {\n",
        "\n",
        "  int n;\n",
        "  int *array;\n",
        "\n",
        "  scanf(\"%d\",&n);\n",
        "\n",
        "  printf(\"The value of n is %d\\n\",n);\n",
        "  array = (int *) malloc (n * sizeof(int));\n",
        "  for(int i = 0;i < n;i++) {\n",
        "    scanf(\"%d\",&array[i]);\n",
        "  }\n",
        "  printf(\"The read values are: \\n\");\n",
        "  for(int i = 0;i < n;i++) {\n",
        "    printf(\"%d \",array[i]);\n",
        "  }\n",
        "  printf(\"\\n\");\n",
        "\n",
        "  unsigned int nPartitions = 1;\n",
        "  for(int i = 0;i < n;i++) {\n",
        "    nPartitions = nPartitions * 2;\n",
        "  }\n",
        "  // printf(\"The number of possible partitions is: %d\\n\",nPartitions);\n",
        "  // Only half of all possible partitions need be examined\n",
        "  // The second half is symmetrical to the first half\n",
        "  nPartitions = nPartitions / 2;\n",
        "\n",
        "  int solutionFound = 0;\n",
        "  int solution = -1;\n",
        "  // Allocate the variables in the device:\n",
        "  // The array with the integer values in the device is called d_array\n",
        "  int *d_array;\n",
        "  cudaMalloc(&d_array, n*sizeof(int));\n",
        "\n",
        "  // Copy the variables from the host to the device\n",
        "  cudaMemcpy(d_array,array,n*sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "  // Allocate on the device an array to keep all the results\n",
        "  int *d_results;\n",
        "  cudaMalloc(&d_results,nPartitions*sizeof(int));\n",
        "// Now invoke the kernel\n",
        "  evaluatePartition<<<(nPartitions+31)/32,32>>>(  n, d_array,d_results) ;\n",
        "  // The array on the host that will contain the results is called results\n",
        "  int *results;\n",
        "  results = (int *) calloc (nPartitions , sizeof(int));\n",
        "// Copy the results from the GPU card to main memory on the host\n",
        "  cudaMemcpy(results,d_results,nPartitions*sizeof(int),cudaMemcpyDeviceToHost);\n",
        "  /*\n",
        "  for(int i = 0;i < nPartitions;i++) {\n",
        "\t printf(\"%d \",results[i]);\n",
        "  }\n",
        "  printf(\"\\n\");\n",
        " */\n",
        "  for(int i = 0;i < nPartitions;i++) {\n",
        "\t  if (results[i] != 0) {\n",
        "\t\t  solutionFound = 1;\n",
        "\t\t  solution = i;\n",
        "\t\t  break;\n",
        "\t  }\n",
        "  }\n",
        "\n",
        "  if (solutionFound == 1) {\n",
        "    printResults(solution, n, array);\n",
        "  }\n",
        "  else {\n",
        "    printf(\"No solution was found.\");\n",
        "  }\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccmfoeLnrBDl",
        "outputId": "bfa1f1cb-9921-44bf-d2f0-b150b3e7a8d9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cudaPartition.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After writing the source code, we can compile it using the nvcc compiler."
      ],
      "metadata": {
        "id": "hVCYslsdrLxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc cudaPartition.cu -o cudaPartition -O3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoEK6KYtrV_d",
        "outputId": "a0535373-5dcf-4b39-dcfb-40f1489c26c0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mcudaPartition.cu(70)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1650-D: result of call is not used\n",
            "    scanf(\"%d\",&n);\n",
            "    ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mcudaPartition.cu(75)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1650-D: result of call is not used\n",
            "      scanf(\"%d\",&array[i]);\n",
            "      ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mcudaPartition.cu(70)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1650-D: result of call is not used\n",
            "    scanf(\"%d\",&n);\n",
            "    ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mcudaPartition.cu(75)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1650-D: result of call is not used\n",
            "      scanf(\"%d\",&array[i]);\n",
            "      ^\n",
            "\n",
            "\u001b[01m\u001b[KcudaPartition.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint main()\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[KcudaPartition.cu:70:6:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kint scanf(const char*, ...)\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   70 | \u001b[01;35m\u001b[K  scanf(\"%d\",&n\u001b[m\u001b[K);\n",
            "      |   \u001b[01;35m\u001b[K~~~^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KcudaPartition.cu:75:6:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kint scanf(const char*, ...)\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   75 | \u001b[01;35m\u001b[K    scanf(\"%d\",&array[i]\u001b[m\u001b[K);\n",
            "      |     \u001b[01;35m\u001b[K~^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a test file. This was used previously when testing the OpenMP code.\n"
      ],
      "metadata": {
        "id": "rIWnapo3rkM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile testNoSolution29.txt\n",
        "29\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "30\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hs84bi3JrqS8",
        "outputId": "d2adc6bb-e2bc-47da-a05b-31012f50ee34"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing testNoSolution29.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we can run and time the executable code on the GPU:"
      ],
      "metadata": {
        "id": "xdaLTj1XrySO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!time ./cudaPartition < testNoSolution29.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hH0P8mxRr2_0",
        "outputId": "a1b8273b-83d3-4443-ea1f-596ef371eea8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value of n is 29\n",
            "The read values are: \n",
            "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 30 \n",
            "No solution was found.\n",
            "real\t0m1.740s\n",
            "user\t0m0.646s\n",
            "sys\t0m0.879s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key portions of the code are:\n",
        "1. Allocating variables in the memory of the GPU.\n",
        "\n",
        "This is done using the function\n",
        " cudaMalloc(&d_array, n*sizeof(int));\n",
        "\n",
        " This function has two parameters:\n",
        " - A pointer to a block of memory\n",
        " - The amount of memory that is needed"
      ],
      "metadata": {
        "id": "EvPgVlBEtFq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to copy from the host memory to the GPU memory the content of the variables that will be used in the computation in the GPU. The function\n",
        "cudaMemcpy(d_array,array,n*sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "This function has four parameters:\n",
        "- The variable that will be receiving the result\n",
        "- The source of the copy\n",
        "- The amount of memory (in bytes) that will be copied\n",
        "- A constant that indicates the direction of the transfer. In this case we are copying from the host to the GPU (device)."
      ],
      "metadata": {
        "id": "nRrwp7CIts6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to specify the code that will be executed on the GPU processors.\n",
        "\n",
        "Specifying the code to be executed on the GPU processors is where CUDA is different from regular C code.\n",
        "\n",
        "There are two main considerations:\n",
        "- Describing the code to be executed on each core on the GPU. To designate a particular function as code that will be executed on the GPU cores, CUDA uses two additional keywords __global__ (this indicates that this code can be called from the host ) and __device__ this indicates that this function can be called from another function executing on the GPU.\n",
        "\n",
        "The function will usually operate on entries on an array.\n",
        "It will be necessary to identify the entry on which this function will operate.\n",
        "This is usually achieved with a line like this:\n",
        "```\n",
        "\n",
        "unsigned int value = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "\n",
        "```\n",
        "The block size is chosen by the programmer.\n",
        "\n",
        "```\n",
        "\n",
        "__global__\n",
        "void evaluatePartition(  int n, int *array,int *result) {\n",
        "  unsigned int value = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "  int sum0s = 0;\n",
        "  int sum1s = 0;\n",
        "  unsigned int mask = 1;\n",
        "  for(int i = 0;i < n;i++) {\n",
        "    if ((mask & value) != 0) {\n",
        "      sum1s = sum1s + array[i];\n",
        "    }\n",
        "    else {\n",
        "      sum0s = sum0s + array[i];\n",
        "    }\n",
        "    mask = mask * 2;\n",
        "  }\n",
        "  if (sum0s == sum1s)\n",
        "     result[value] = 1;\n",
        "  else\n",
        "     result[value] = 0;\n",
        "  // printf(\"%d %d \\n\",value,result[value]);\n",
        "\n",
        "}\n",
        "```\n",
        "- The second consideration is how to call this function. NVIDIA introduced a new syntactic element into CUDA: Chevrons. Chevrons indicate that this function is meant to be executed on the GPU. Two values are passed between the Chevrons:\n",
        "```\n",
        " evaluatePartition<<<(nPartitions+31)/32,32>>>(  n, d_array,d_results) ;\n",
        "```\n",
        "The first parameter is the number of blocks that will be required for the execution of the program as an expression based on the size of the problem. Frequently, the size of the problem is the size of the array that will be operated upon. The second parameter is the block size."
      ],
      "metadata": {
        "id": "jtBUEeYWuakm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the results are copied back to the host:\n",
        "\n",
        "```\n",
        "cudaMemcpy(results,d_results,nPartitions*sizeof(int),cudaMemcpyDeviceToHost);\n",
        "```\n",
        "\n",
        "The only change with respect to the previous usage of cudaMemcpy is the direction. Now one is copying from the GPU (device) to the host."
      ],
      "metadata": {
        "id": "ksl9ckp7dow8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create some additional test files so that we can observe how the execution times increase as the problem size grows.\n"
      ],
      "metadata": {
        "id": "zUHlbA5gg35T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test24.txt\n",
        "24\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "23"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUqWlL5vg-NS",
        "outputId": "43abd5e4-527f-4219-ed9e-62646421fdc4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test24.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test25.txt\n",
        "25\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "24"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghuamEBmhGOb",
        "outputId": "0c4677c5-8b5f-4c20-8881-26ff18ecee8f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test25.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test26.txt\n",
        "26\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3ndmPfEhM6c",
        "outputId": "0660fd0f-be2a-4a77-d1dd-7948e1e4aac9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test26.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test27.txt\n",
        "27\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "26"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCbnXqhShV2j",
        "outputId": "73b5f5a4-c87e-403c-e4a4-4cb2aa1c4616"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test27.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!time ./cudaPartition < test24.txt\n",
        "!time ./cudaPartition < test25.txt\n",
        "!time ./cudaPartition < test26.txt\n",
        "!time ./cudaPartition < test27.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zs44x78hYoD",
        "outputId": "e160580a-4f56-4c47-9e54-c3b234f1542d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value of n is 24\n",
            "The read values are: \n",
            "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 23 \n",
            "No solution was found.\n",
            "real\t0m0.186s\n",
            "user\t0m0.040s\n",
            "sys\t0m0.135s\n",
            "The value of n is 25\n",
            "The read values are: \n",
            "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 24 \n",
            "No solution was found.\n",
            "real\t0m0.248s\n",
            "user\t0m0.063s\n",
            "sys\t0m0.163s\n",
            "The value of n is 26\n",
            "The read values are: \n",
            "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 25 \n",
            "No solution was found.\n",
            "real\t0m0.315s\n",
            "user\t0m0.090s\n",
            "sys\t0m0.205s\n",
            "The value of n is 27\n",
            "The read values are: \n",
            "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 26 \n",
            "No solution was found.\n",
            "real\t0m0.485s\n",
            "user\t0m0.175s\n",
            "sys\t0m0.292s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Profiling the code\n",
        "NVIDA has created a profiler that can be used to find where the time is being spent during the execution of the program. The command is called nvprof."
      ],
      "metadata": {
        "id": "ps8lajSmgeUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./cudaPartition < test24.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNIndqg2fbaS",
        "outputId": "5791d135-bcb9-4686-cf6c-6ff7fe72bcd4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value of n is 24\n",
            "The read values are: \n",
            "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 23 \n",
            "==1660== NVPROF is profiling process 1660, command: ./cudaPartition\n",
            "==1660== No solution was found.Profiling application: ./cudaPartition\n",
            "==1660== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  26.499ms         1  26.499ms  26.499ms  26.499ms  [CUDA memcpy DtoH]\n",
            "                    0.00%  1.1840us         1  1.1840us  1.1840us  1.1840us  [CUDA memcpy HtoD]\n",
            "      API calls:   68.33%  86.300ms         2  43.150ms  120.43us  86.180ms  cudaMalloc\n",
            "                   22.18%  28.010ms         2  14.005ms  29.609us  27.980ms  cudaMemcpy\n",
            "                    9.24%  11.664ms         1  11.664ms  11.664ms  11.664ms  cudaLaunchKernel\n",
            "                    0.23%  295.37us       114  2.5900us     106ns  174.64us  cuDeviceGetAttribute\n",
            "                    0.01%  12.173us         1  12.173us  12.173us  12.173us  cuDeviceGetName\n",
            "                    0.01%  6.8230us         1  6.8230us  6.8230us  6.8230us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.8060us         3     602ns     234ns  1.3370us  cuDeviceGetCount\n",
            "                    0.00%  1.0790us         2     539ns     129ns     950ns  cuDeviceGet\n",
            "                    0.00%     643ns         1     643ns     643ns     643ns  cuModuleGetLoadingMode\n",
            "                    0.00%     603ns         1     603ns     603ns     603ns  cuDeviceTotalMem\n",
            "                    0.00%     311ns         1     311ns     311ns     311ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./cudaPartition < testNoSolution29.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEwQDEmGgKDi",
        "outputId": "e0845fc3-f195-451e-b2a7-9b169a2a1232"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value of n is 29\n",
            "The read values are: \n",
            "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 30 \n",
            "==1672== NVPROF is profiling process 1672, command: ./cudaPartition\n",
            "No solution was found.==1672== Profiling application: ./cudaPartition\n",
            "==1672== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  901.54ms         1  901.54ms  901.54ms  901.54ms  [CUDA memcpy DtoH]\n",
            "                    0.00%  1.1840us         1  1.1840us  1.1840us  1.1840us  [CUDA memcpy HtoD]\n",
            "      API calls:   90.17%  902.95ms         2  451.47ms  19.011us  902.93ms  cudaMemcpy\n",
            "                    8.90%  89.090ms         2  44.545ms  202.67us  88.888ms  cudaMalloc\n",
            "                    0.91%  9.0941ms         1  9.0941ms  9.0941ms  9.0941ms  cudaLaunchKernel\n",
            "                    0.02%  172.75us       114  1.5150us     115ns  67.447us  cuDeviceGetAttribute\n",
            "                    0.00%  15.780us         1  15.780us  15.780us  15.780us  cuDeviceGetName\n",
            "                    0.00%  8.1240us         1  8.1240us  8.1240us  8.1240us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.5600us         3     520ns     146ns  1.1280us  cuDeviceGetCount\n",
            "                    0.00%  1.4440us         2     722ns     314ns  1.1300us  cuDeviceGet\n",
            "                    0.00%     510ns         1     510ns     510ns     510ns  cuDeviceTotalMem\n",
            "                    0.00%     434ns         1     434ns     434ns     434ns  cuModuleGetLoadingMode\n",
            "                    0.00%     295ns         1     295ns     295ns     295ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the back of your summary of the readings for this week, fill the following form with the execution times:\n",
        "\n",
        "| File name  | Execution Time |\n",
        ":------------| -----------:|\n",
        "| test24.txt |  |0.186\n",
        "| test25.txt |  |0.248\n",
        "| test26.txt |  |0.315\n",
        "| test27.txt |  |0.485\n",
        "| testNoSolution29.txt | 910.64 |\n",
        "\n",
        "Look at the information provide by the profiler.\n",
        "Where is most of the execution time being spent?\n",
        "\n",
        "If a reduction could be performed efficiently on the GPU card, and you wouldn't have to copy back the entire d_results array,  would this reduce the total execution time?"
      ],
      "metadata": {
        "id": "PPuWx3x4jW9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CGkTYS6Fjo6j"
      }
    }
  ]
}